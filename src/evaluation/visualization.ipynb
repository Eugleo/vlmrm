{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "\n",
    "Install dependencies:\n",
    "\n",
    "```shell\n",
    "pip install jupyter altair altair_saver polars pyarrow anywidget ipywidgets\n",
    "```\n",
    "\n",
    "Then, as per the [CAIS tutorial](https://cluster.safe.ai/#jupyter-notebooks-on-the-cluster), start a new interactive node:\n",
    "\n",
    "```shell\n",
    "srun  --pty bash\n",
    "```\n",
    "\n",
    "Then note the port number from:\n",
    "\n",
    "```shell\n",
    "unset XDG_RUNTIME_DIR\n",
    "export NODEPORT=$(( $RANDOM + 1024 ))\n",
    "echo $NODEPORT\n",
    "jupyter notebook --no-browser --port=$NODEPORT\n",
    "```\n",
    "\n",
    "Then on you local machine run (filling in the port from above):\n",
    "\n",
    "```shell\n",
    "export NODEPORT=####\n",
    "ssh -t -t [your ssh alias for cais cluster] -L ${NODEPORT}:localhost:${NODEPORT} ssh -N compute-permanent-node-990 -L ${NODEPORT}:localhost:${NODEPORT}\n",
    "```\n",
    "\n",
    "Now, open your local browser and enter the URL from jupyter (`http://localhost:19303/?token=cb...`), or open VSCode and under Select Kernel choose \"Existing Jupyter Server\" and input it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import altair as alt\n",
    "import polars as pl\n",
    "import sklearn.metrics as skm\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import HTML, Dropdown, HBox, Label, Output, VBox, Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helper function that are not very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _max_prob(df: pl.DataFrame, col: str):\n",
    "    return df.group_by([\"video\", \"task\", \"model\", \"reward\"]).agg(\n",
    "        # Extract the label with the highest probability\n",
    "        pl.col(\"label\").sort_by(col).last()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the latest experiment in `out` is loaded. You can change this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest experiment\n",
    "experiments = Path(\"../../out\").iterdir()\n",
    "latest_experiment = sorted(experiments, key=lambda d: d.stat().st_mtime)[-1]\n",
    "experiment_dir = latest_experiment\n",
    "\n",
    "df = pl.read_csv(experiment_dir / \"results.csv\")\n",
    "\n",
    "predicted_labels = _max_prob(df, \"probability\").rename({\"label\": \"predicted_label\"})\n",
    "true_labels = _max_prob(df, \"true_probability\").rename({\"label\": \"true_label\"})\n",
    "predictions = predicted_labels.join(\n",
    "    true_labels, on=[\"video\", \"task\", \"model\", \"reward\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the table `predictions`, containing the predictions of all model+reward combinations for all tasks and videos. Most visualizations will be okay with using this as a base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>video</th><th>task</th><th>model</th><th>reward</th><th>predicted_label</th><th>true_label</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;/data/datasets…</td><td>&quot;room_detection…</td><td>&quot;viclip&quot;</td><td>&quot;logit&quot;</td><td>&quot;bedroom&quot;</td><td>&quot;dressing_room&quot;</td></tr><tr><td>&quot;/data/datasets…</td><td>&quot;room_detection…</td><td>&quot;viclip&quot;</td><td>&quot;projection_0.2…</td><td>&quot;stairs&quot;</td><td>&quot;stairs&quot;</td></tr><tr><td>&quot;/data/datasets…</td><td>&quot;room_detection…</td><td>&quot;viclip&quot;</td><td>&quot;projection_0.7…</td><td>&quot;bedroom&quot;</td><td>&quot;hall&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 6)\n",
       "┌────────────────────┬────────────────┬────────┬─────────────────┬─────────────────┬───────────────┐\n",
       "│ video              ┆ task           ┆ model  ┆ reward          ┆ predicted_label ┆ true_label    │\n",
       "│ ---                ┆ ---            ┆ ---    ┆ ---             ┆ ---             ┆ ---           │\n",
       "│ str                ┆ str            ┆ str    ┆ str             ┆ str             ┆ str           │\n",
       "╞════════════════════╪════════════════╪════════╪═════════════════╪═════════════════╪═══════════════╡\n",
       "│ /data/datasets/hab ┆ room_detection ┆ viclip ┆ logit           ┆ bedroom         ┆ dressing_room │\n",
       "│ itat_recording…    ┆                ┆        ┆                 ┆                 ┆               │\n",
       "│ /data/datasets/hab ┆ room_detection ┆ viclip ┆ projection_0.25 ┆ stairs          ┆ stairs        │\n",
       "│ itat_recording…    ┆                ┆        ┆                 ┆                 ┆               │\n",
       "│ /data/datasets/hab ┆ room_detection ┆ viclip ┆ projection_0.75 ┆ bedroom         ┆ hall          │\n",
       "│ itat_recording…    ┆                ┆        ┆                 ┆                 ┆               │\n",
       "└────────────────────┴────────────────┴────────┴─────────────────┴─────────────────┴───────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on `predictions`, we can calculate metrics for each task, model, reward combination. Feel free to add more here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>task</th><th>model</th><th>reward</th><th>f1</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;room_detection…</td><td>&quot;viclip&quot;</td><td>&quot;projection_0.7…</td><td>0.414063</td></tr><tr><td>&quot;object_detecti…</td><td>&quot;s3d&quot;</td><td>&quot;logit&quot;</td><td>0.1639</td></tr><tr><td>&quot;room_detection…</td><td>&quot;s3d&quot;</td><td>&quot;logit&quot;</td><td>0.245357</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌──────────────────┬────────┬─────────────────┬──────────┐\n",
       "│ task             ┆ model  ┆ reward          ┆ f1       │\n",
       "│ ---              ┆ ---    ┆ ---             ┆ ---      │\n",
       "│ str              ┆ str    ┆ str             ┆ f64      │\n",
       "╞══════════════════╪════════╪═════════════════╪══════════╡\n",
       "│ room_detection   ┆ viclip ┆ projection_0.75 ┆ 0.414063 │\n",
       "│ object_detection ┆ s3d    ┆ logit           ┆ 0.1639   │\n",
       "│ room_detection   ┆ s3d    ┆ logit           ┆ 0.245357 │\n",
       "└──────────────────┴────────┴─────────────────┴──────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A wrapper around a sklearn metric function\n",
    "def f1(group: pl.Series):\n",
    "    # group is a pl.Series object with two named fields, true_label and predicted_label\n",
    "    # we can access those fields using group.struct.field\n",
    "    return skm.f1_score(\n",
    "        y_true=group.struct.field(\"true_label\").to_numpy(),\n",
    "        y_pred=group.struct.field(\"predicted_label\").to_numpy(),\n",
    "        average=\"macro\",\n",
    "    )\n",
    "\n",
    "\n",
    "# A helper function used to extract label colums from the dataframe,\n",
    "# package them as structs, and then map matric_fun over them\n",
    "def compute_metric(metric_fun):\n",
    "    return pl.struct(\"true_label\", \"predicted_label\").map_batches(metric_fun).first()\n",
    "\n",
    "\n",
    "metrics = predictions.group_by(\"task\", \"model\", \"reward\").agg(\n",
    "    f1=compute_metric(f1)\n",
    "    # ...add more here!\n",
    ")\n",
    "\n",
    "metrics.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a table with metrics, we can plot it. We use `altair` because it allows for the type of interactivity we need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-cb129bbc104a4cfdaffa616b7f3eb3ff.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-cb129bbc104a4cfdaffa616b7f3eb3ff.vega-embed details,\n",
       "  #altair-viz-cb129bbc104a4cfdaffa616b7f3eb3ff.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-cb129bbc104a4cfdaffa616b7f3eb3ff\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-cb129bbc104a4cfdaffa616b7f3eb3ff\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-cb129bbc104a4cfdaffa616b7f3eb3ff\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-dba3fee8d08d9c222df258a1c7bb2413\"}, \"facet\": {\"column\": {\"field\": \"model\", \"type\": \"nominal\"}, \"row\": {\"field\": \"task\", \"type\": \"nominal\"}}, \"spec\": {\"mark\": {\"type\": \"bar\", \"width\": 10}, \"encoding\": {\"color\": {\"field\": \"model\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"f1\", \"type\": \"quantitative\"}, {\"field\": \"model\", \"type\": \"nominal\"}, {\"field\": \"reward\", \"type\": \"nominal\"}], \"x\": {\"field\": \"reward\", \"type\": \"nominal\"}, \"y\": {\"field\": \"f1\", \"type\": \"quantitative\"}}}, \"title\": \"F1 score\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-dba3fee8d08d9c222df258a1c7bb2413\": [{\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.75\", \"f1\": 0.4140627270586231}, {\"task\": \"object_detection\", \"model\": \"s3d\", \"reward\": \"logit\", \"f1\": 0.16390022675736962}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"logit\", \"f1\": 0.24535684096123653}, {\"task\": \"object_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.0\", \"f1\": 0.03210272873194221}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.0\", \"f1\": 0.01085972850678733}, {\"task\": \"object_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.25\", \"f1\": 0.372562358276644}, {\"task\": \"object_detection\", \"model\": \"clip\", \"reward\": \"projection_0.0\", \"f1\": 0.2632629045672524}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_0.0\", \"f1\": 0.6218423356432408}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.50\", \"f1\": 0.6730623035501083}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"logit\", \"f1\": 0.6187537575546626}, {\"task\": \"object_detection\", \"model\": \"clip\", \"reward\": \"logit\", \"f1\": 0.2667793880837359}, {\"task\": \"object_detection\", \"model\": \"gpt4\", \"reward\": \"default\", \"f1\": 0.38961469240677704}, {\"task\": \"object_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.75\", \"f1\": 0.28652970117664184}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.0\", \"f1\": 0.7193092943092944}, {\"task\": \"object_detection\", \"model\": \"clip\", \"reward\": \"projection_0.50\", \"f1\": 0.1812860779843454}, {\"task\": \"room_detection\", \"model\": \"gpt4\", \"reward\": \"default\", \"f1\": 0.7232209819166341}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_0.50\", \"f1\": 0.6218423356432408}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_1.0\", \"f1\": 0.2567987567987568}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"logit\", \"f1\": 0.7193092943092944}, {\"task\": \"object_detection\", \"model\": \"clip\", \"reward\": \"projection_1.0\", \"f1\": 0.061583190167834014}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.50\", \"f1\": 0.005628517823639775}, {\"task\": \"object_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.50\", \"f1\": 0.09113300492610839}, {\"task\": \"object_detection\", \"model\": \"clip\", \"reward\": \"projection_0.25\", \"f1\": 0.2496894409937888}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_0.25\", \"f1\": 0.6218423356432408}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_1.0\", \"f1\": 0.005628517823639775}, {\"task\": \"object_detection\", \"model\": \"s3d\", \"reward\": \"projection_1.0\", \"f1\": 0.017006802721088433}, {\"task\": \"object_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.0\", \"f1\": 0.4463735999450285}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_1.0\", \"f1\": 0.049960064997658975}, {\"task\": \"object_detection\", \"model\": \"viclip\", \"reward\": \"logit\", \"f1\": 0.4463735999450285}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_0.75\", \"f1\": 0.6325038963048013}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.25\", \"f1\": 0.6688811188811189}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.75\", \"f1\": 0.0074142724745134385}, {\"task\": \"object_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.75\", \"f1\": 0.03210272873194221}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.25\", \"f1\": 0.005628517823639775}, {\"task\": \"object_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.25\", \"f1\": 0.0035714285714285718}, {\"task\": \"object_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.50\", \"f1\": 0.39145880574452}, {\"task\": \"object_detection\", \"model\": \"viclip\", \"reward\": \"projection_1.0\", \"f1\": 0.10501930501930502}, {\"task\": \"object_detection\", \"model\": \"clip\", \"reward\": \"projection_0.75\", \"f1\": 0.0896358543417367}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_metric(metric_name):\n",
    "    return (\n",
    "        alt.Chart(metrics.to_pandas())\n",
    "        .mark_bar(width=10)\n",
    "        .encode(\n",
    "            x=\"reward\",\n",
    "            y=metric_name,\n",
    "            color=\"model\",\n",
    "            tooltip=[metric_name, \"model\", \"reward\"],\n",
    "        )\n",
    "        .facet(column=\"model\", row=\"task\")\n",
    "        .properties(title=\"F1 score\")\n",
    "    )\n",
    "\n",
    "\n",
    "plot_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the plotting needs will probably be taken care of by the above, or small variations of it. Below we have the interactive confusion matrix; the code there shouldn't be too important to fully understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart(task, model):\n",
    "    # A table of best model+reard combination for each model and task\n",
    "    best_models = metrics.group_by(\"task\", \"model\").agg(\n",
    "        pl.col(\"reward\").sort_by(\"f1\", descending=True).first()\n",
    "    )\n",
    "\n",
    "    # The predictions table filtered to only contain the best models\n",
    "    # i.e. exactly one model+reward per task\n",
    "    best_model_predicitons = predictions.filter(\n",
    "        pl.col(\"task\") == task, pl.col(\"model\") == model\n",
    "    ).join(best_models, on=[\"task\", \"model\", \"reward\"], how=\"semi\")\n",
    "\n",
    "    # A normal confusion matrix\n",
    "    confusion_matrix = best_model_predicitons.group_by(\n",
    "        \"model\", \"reward\", \"true_label\", \"predicted_label\"\n",
    "    ).agg(pl.len().alias(\"count\"))\n",
    "\n",
    "    # Needed to register the click events\n",
    "    selection = alt.selection_point(\n",
    "        fields=[\"true_label\", \"predicted_label\"], name=\"selection\"\n",
    "    )\n",
    "\n",
    "    # Base chart to which we'll add layers later\n",
    "    base = (\n",
    "        alt.Chart(confusion_matrix.to_pandas())\n",
    "        .encode(\n",
    "            x=\"predicted_label\",\n",
    "            y=\"true_label\",\n",
    "        )\n",
    "        .properties(title=f\"{model}, {task}\")\n",
    "    )\n",
    "\n",
    "    # Heatmap layer\n",
    "    heatmap = base.mark_rect().encode(\n",
    "        color=alt.Color(\"count\").scale(scheme=\"blues\"),\n",
    "        tooltip=[\"true_label\", \"predicted_label\", \"count\"],\n",
    "    )\n",
    "\n",
    "    # Diagonal frames layer\n",
    "    labels = confusion_matrix[\"true_label\"].unique()\n",
    "    diag_df = pl.DataFrame({\"predicted_label\": labels, \"true_label\": labels})\n",
    "    diagonal = (\n",
    "        alt.Chart(pl.DataFrame(diag_df).to_pandas())\n",
    "        .mark_rect(stroke=\"black\", strokeWidth=1, fillOpacity=0)\n",
    "        .encode(x=\"predicted_label\", y=\"true_label\")\n",
    "    )\n",
    "\n",
    "    # Add the layers together and also add the click-selector from eariler\n",
    "    # Returning this would give us a normal chart, like the one above\n",
    "    chart = (heatmap + diagonal).add_params(selection)\n",
    "\n",
    "    # Wrap the chart in a Jupyter widget\n",
    "    jchart = alt.JupyterChart(chart)\n",
    "\n",
    "    # This is the vertical box the videos will live in\n",
    "    videos_widget = VBox()\n",
    "\n",
    "    # Click callback\n",
    "    def on_select(change):\n",
    "        if change.new.value is None:\n",
    "            return\n",
    "\n",
    "        videos: list[str] = []\n",
    "        for sel in change.new.value:\n",
    "            # Get a list of videos that correspond to the cell that was clicked on\n",
    "            videos.extend(\n",
    "                best_model_predicitons.filter(\n",
    "                    pl.col(\"model\") == model,\n",
    "                    pl.col(\"task\") == task,\n",
    "                    pl.col(\"true_label\") == sel[\"true_label\"],\n",
    "                    pl.col(\"predicted_label\") == sel[\"predicted_label\"],\n",
    "                )\n",
    "                .get_column(\"video\")\n",
    "                .to_list()\n",
    "            )\n",
    "        # Load the videos based on the paths above, and put them into a flexbox\n",
    "        videos = [VBox([Video.from_file(path), Label(\"👆 \" + path)]) for path in videos]\n",
    "        videos_widget.children = videos\n",
    "\n",
    "    # Whenever the selection in the chart changes, call the callback above\n",
    "    jchart.selections.observe(on_select, [\"selection\"])\n",
    "\n",
    "    return HBox([jchart, videos_widget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the cell below, you can pick the model and task combination and also click the cells in the matrix to see which videos ended up in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20a0e81c7d6419ca34f0cf838d23077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Task:', options=('object_detection', 'room_detection'), value='object_det…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f99383bbbdf45b1ac496f5881e90eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a task selection dropdown\n",
    "tasks = predictions.get_column(\"task\").unique().sort()\n",
    "task_dropdown = Dropdown(\n",
    "    options=tasks,\n",
    "    value=tasks[0],\n",
    "    description=\"Task:\",\n",
    ")\n",
    "\n",
    "# Create a model selection dropdown\n",
    "models = predictions.get_column(\"model\").unique().sort()\n",
    "model_dropdown = Dropdown(\n",
    "    options=models,\n",
    "    value=models[0],\n",
    "    description=\"Model:\",\n",
    ")\n",
    "\n",
    "# Create an \"output\", a sort of a canvas that we can render things into\n",
    "# This is needed for the live updates whenever the dropdowns change\n",
    "output = Output()\n",
    "\n",
    "\n",
    "def on_change(_change):\n",
    "    with output:\n",
    "        # Clear the canvas and render the new plot\n",
    "        clear_output()\n",
    "        display(chart(task_dropdown.value, model_dropdown.value))\n",
    "\n",
    "\n",
    "model_dropdown.observe(on_change, names=[\"value\"])\n",
    "task_dropdown.observe(on_change, names=[\"value\"])\n",
    "\n",
    "# Render the dropdowns in Jupyter\n",
    "display(VBox([task_dropdown, model_dropdown]))\n",
    "\n",
    "with output:\n",
    "    # Redner the chart into the output\n",
    "    display(chart(task_dropdown.value, model_dropdown.value))\n",
    "\n",
    "# Render the output in Jupyter\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
