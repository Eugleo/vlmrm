{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "\n",
    "Install dependencies:\n",
    "\n",
    "```shell\n",
    "pip install jupyter altair altair_saver polars pyarrow anywidget ipywidgets\n",
    "```\n",
    "\n",
    "Then, as per the [CAIS tutorial](https://cluster.safe.ai/#jupyter-notebooks-on-the-cluster), start a new interactive node:\n",
    "\n",
    "```shell\n",
    "srun --partition=single --pty bash\n",
    "```\n",
    "\n",
    "Then note the port number from:\n",
    "\n",
    "```shell\n",
    "unset XDG_RUNTIME_DIR\n",
    "export NODEPORT=$(( $RANDOM + 1024 ))\n",
    "echo $NODEPORT\n",
    "jupyter notebook --no-browser --port=$NODEPORT\n",
    "```\n",
    "\n",
    "Then on you local machine run (filling in the port from above).\n",
    "\n",
    "```shell\n",
    "export NODEPORT=####\n",
    "ssh -t -t [your ssh alias for cais cluster] -L ${NODEPORT}:localhost:${NODEPORT} ssh -N compute-permanent-node-990 -L ${NODEPORT}:localhost:${NODEPORT}\n",
    "```\n",
    "\n",
    "Now, open your local browser and enter the URL from jupyter (`http://localhost:19303/?token=cb...`), or open VSCode and under Select Kernel choose \"Existing Jupyter Server\" and input it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import altair as alt\n",
    "import polars as pl\n",
    "import sklearn.metrics as skm\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import HTML, Dropdown, HBox, Label, Output, VBox, Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helper function that are not very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metadata_cols(df: pl.DataFrame):\n",
    "    tasks = df.get_column(\"task\").unique().to_list()\n",
    "    default = [\n",
    "        \"path\",\n",
    "        \"task\",\n",
    "        \"model\",\n",
    "        \"reward\",\n",
    "        \"label\",\n",
    "        \"true_probability\",\n",
    "        \"probability\",\n",
    "    ]\n",
    "    return [c for c in df.columns if c not in tasks + default]\n",
    "\n",
    "\n",
    "def _max_prob(df: pl.DataFrame, col: str):\n",
    "    tasks = df.get_column(\"task\").unique().to_list()\n",
    "    return df.group_by([\"path\", \"task\", \"model\", \"reward\"]).agg(\n",
    "        # Extract the label with the highest probability\n",
    "        pl.col(\"label\").sort_by(col).last(),\n",
    "        pl.col(_metadata_cols(df)).first(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the latest experiment in `out` is loaded. You can change this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest experiment\n",
    "experiments = Path(\"../../out\").iterdir()\n",
    "latest_experiment = sorted(experiments, key=lambda d: d.stat().st_mtime)[-1]\n",
    "experiment_dir = latest_experiment\n",
    "\n",
    "df = pl.read_csv(experiment_dir / \"results.csv\")\n",
    "\n",
    "predicted_labels = _max_prob(df, \"probability\").rename({\"label\": \"predicted_label\"})\n",
    "true_labels = _max_prob(df, \"true_probability\").rename({\"label\": \"true_label\"})\n",
    "predictions = predicted_labels.join(\n",
    "    true_labels, on=[\"path\", \"task\", \"model\", \"reward\"] + _metadata_cols(df)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the table `predictions`, containing the predictions of all model+reward combinations for all tasks and videos. Most visualizations will be okay with using this as a base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>path</th><th>task</th><th>model</th><th>reward</th><th>predicted_label</th><th>object_detection</th><th>is_photorealistic</th><th>true_label</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>bool</td><td>str</td></tr></thead><tbody><tr><td>&quot;/data/datasets…</td><td>&quot;room_detection…</td><td>&quot;gpt4&quot;</td><td>&quot;default&quot;</td><td>&quot;bathroom&quot;</td><td>&quot;toilet&quot;</td><td>true</td><td>&quot;bathroom&quot;</td></tr><tr><td>&quot;/data/datasets…</td><td>&quot;room_detection…</td><td>&quot;gpt4&quot;</td><td>&quot;default&quot;</td><td>&quot;bathroom&quot;</td><td>&quot;toilet&quot;</td><td>true</td><td>&quot;bathroom&quot;</td></tr><tr><td>&quot;/data/datasets…</td><td>&quot;room_detection…</td><td>&quot;gpt4&quot;</td><td>&quot;default&quot;</td><td>&quot;bathroom&quot;</td><td>&quot;toilet&quot;</td><td>true</td><td>&quot;bathroom&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 8)\n",
       "┌─────────────┬─────────────┬───────┬─────────┬─────────────┬────────────┬────────────┬────────────┐\n",
       "│ path        ┆ task        ┆ model ┆ reward  ┆ predicted_l ┆ object_det ┆ is_photore ┆ true_label │\n",
       "│ ---         ┆ ---         ┆ ---   ┆ ---     ┆ abel        ┆ ection     ┆ alistic    ┆ ---        │\n",
       "│ str         ┆ str         ┆ str   ┆ str     ┆ ---         ┆ ---        ┆ ---        ┆ str        │\n",
       "│             ┆             ┆       ┆         ┆ str         ┆ str        ┆ bool       ┆            │\n",
       "╞═════════════╪═════════════╪═══════╪═════════╪═════════════╪════════════╪════════════╪════════════╡\n",
       "│ /data/datas ┆ room_detect ┆ gpt4  ┆ default ┆ bathroom    ┆ toilet     ┆ true       ┆ bathroom   │\n",
       "│ ets/habitat ┆ ion         ┆       ┆         ┆             ┆            ┆            ┆            │\n",
       "│ _recording… ┆             ┆       ┆         ┆             ┆            ┆            ┆            │\n",
       "│ /data/datas ┆ room_detect ┆ gpt4  ┆ default ┆ bathroom    ┆ toilet     ┆ true       ┆ bathroom   │\n",
       "│ ets/habitat ┆ ion         ┆       ┆         ┆             ┆            ┆            ┆            │\n",
       "│ _recording… ┆             ┆       ┆         ┆             ┆            ┆            ┆            │\n",
       "│ /data/datas ┆ room_detect ┆ gpt4  ┆ default ┆ bathroom    ┆ toilet     ┆ true       ┆ bathroom   │\n",
       "│ ets/habitat ┆ ion         ┆       ┆         ┆             ┆            ┆            ┆            │\n",
       "│ _recording… ┆             ┆       ┆         ┆             ┆            ┆            ┆            │\n",
       "└─────────────┴─────────────┴───────┴─────────┴─────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on `predictions`, we can calculate metrics for each task, model, reward combination. Feel free to add more here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>task</th><th>model</th><th>reward</th><th>f1</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;room_detection…</td><td>&quot;gpt4&quot;</td><td>&quot;default&quot;</td><td>1.0</td></tr><tr><td>&quot;clip_through_d…</td><td>&quot;gpt4&quot;</td><td>&quot;default&quot;</td><td>0.653333</td></tr><tr><td>&quot;room_detection…</td><td>&quot;clip&quot;</td><td>&quot;logit&quot;</td><td>0.699662</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌────────────────────────┬───────┬─────────┬──────────┐\n",
       "│ task                   ┆ model ┆ reward  ┆ f1       │\n",
       "│ ---                    ┆ ---   ┆ ---     ┆ ---      │\n",
       "│ str                    ┆ str   ┆ str     ┆ f64      │\n",
       "╞════════════════════════╪═══════╪═════════╪══════════╡\n",
       "│ room_detection         ┆ gpt4  ┆ default ┆ 1.0      │\n",
       "│ clip_through_detection ┆ gpt4  ┆ default ┆ 0.653333 │\n",
       "│ room_detection         ┆ clip  ┆ logit   ┆ 0.699662 │\n",
       "└────────────────────────┴───────┴─────────┴──────────┘"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A wrapper around a sklearn metric function\n",
    "def f1(group: pl.Series):\n",
    "    # group is a pl.Series object with two named fields, true_label and predicted_label\n",
    "    # we can access those fields using group.struct.field\n",
    "    return skm.f1_score(\n",
    "        y_true=group.struct.field(\"true_label\").to_numpy(),\n",
    "        y_pred=group.struct.field(\"predicted_label\").to_numpy(),\n",
    "        average=\"macro\",\n",
    "    )\n",
    "\n",
    "\n",
    "# A helper function used to extract label colums from the dataframe,\n",
    "# package them as structs, and then map matric_fun over them\n",
    "def compute_metric(metric_fun):\n",
    "    return pl.struct(\"true_label\", \"predicted_label\").map_batches(metric_fun).first()\n",
    "\n",
    "\n",
    "metrics = predictions.group_by(\"task\", \"model\", \"reward\").agg(\n",
    "    f1=compute_metric(f1),\n",
    "    # ...add more here!\n",
    ")\n",
    "\n",
    "metrics.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a table with metrics, we can plot it. We use `altair` because it allows for the type of interactivity we need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-19d96749d86a4bcaba4bff269bbdc10d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-19d96749d86a4bcaba4bff269bbdc10d.vega-embed details,\n",
       "  #altair-viz-19d96749d86a4bcaba4bff269bbdc10d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-19d96749d86a4bcaba4bff269bbdc10d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-19d96749d86a4bcaba4bff269bbdc10d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-19d96749d86a4bcaba4bff269bbdc10d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-1be8e34c9b4f2792fa4df9b2646b2d8e\"}, \"facet\": {\"column\": {\"field\": \"model\", \"type\": \"nominal\"}, \"row\": {\"field\": \"task\", \"type\": \"nominal\"}}, \"spec\": {\"mark\": {\"type\": \"bar\", \"width\": 10}, \"encoding\": {\"color\": {\"field\": \"model\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"f1\", \"type\": \"quantitative\"}, {\"field\": \"model\", \"type\": \"nominal\"}, {\"field\": \"reward\", \"type\": \"nominal\"}], \"x\": {\"field\": \"reward\", \"type\": \"nominal\"}, \"y\": {\"field\": \"f1\", \"type\": \"quantitative\"}}}, \"title\": \"F1 score\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-1be8e34c9b4f2792fa4df9b2646b2d8e\": [{\"task\": \"room_detection\", \"model\": \"gpt4\", \"reward\": \"default\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 1.0}, {\"task\": \"clip_through_detection\", \"model\": \"gpt4\", \"reward\": \"default\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.6533333333333333}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"logit\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.6996619738011353}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_0.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.6996619738011353}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_0.25\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.6726472587124638}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_0.50\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.6325647418556503}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_0.75\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.5456347972739587}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"projection_1.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.20286900561938362}, {\"task\": \"clip_through_detection\", \"model\": \"clip\", \"reward\": \"logit\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.46760982874162327}, {\"task\": \"clip_through_detection\", \"model\": \"clip\", \"reward\": \"projection_0.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.46760982874162327}, {\"task\": \"clip_through_detection\", \"model\": \"clip\", \"reward\": \"projection_0.25\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.6153846153846154}, {\"task\": \"clip_through_detection\", \"model\": \"clip\", \"reward\": \"projection_0.50\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.616745283018868}, {\"task\": \"clip_through_detection\", \"model\": \"clip\", \"reward\": \"projection_0.75\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4628099173553719}, {\"task\": \"clip_through_detection\", \"model\": \"clip\", \"reward\": \"projection_1.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4628099173553719}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"logit\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.75269968593498}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.75269968593498}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.25\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.7080522962875905}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.50\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.6774047784393858}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.75\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4652569624833493}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"projection_1.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.13040896127951077}, {\"task\": \"clip_through_detection\", \"model\": \"viclip\", \"reward\": \"logit\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.5604395604395604}, {\"task\": \"clip_through_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.5604395604395604}, {\"task\": \"clip_through_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.25\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.5286439448875997}, {\"task\": \"clip_through_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.50\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.511574354623682}, {\"task\": \"clip_through_detection\", \"model\": \"viclip\", \"reward\": \"projection_0.75\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4583333333333333}, {\"task\": \"clip_through_detection\", \"model\": \"viclip\", \"reward\": \"projection_1.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4628099173553719}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"logit\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.33609513520550655}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.00851581508515815}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.25\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.00851581508515815}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.50\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.00851581508515815}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.75\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.00851581508515815}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"projection_1.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.00851581508515815}, {\"task\": \"clip_through_detection\", \"model\": \"s3d\", \"reward\": \"logit\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.1408315308819995}, {\"task\": \"clip_through_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4628099173553719}, {\"task\": \"clip_through_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.25\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4628099173553719}, {\"task\": \"clip_through_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.50\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4628099173553719}, {\"task\": \"clip_through_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.75\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4628099173553719}, {\"task\": \"clip_through_detection\", \"model\": \"s3d\", \"reward\": \"projection_1.0\", \"object_detection\": \"toilet\", \"is_photorealistic\": true, \"f1\": 0.4628099173553719}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_metric(metric_name):\n",
    "    return (\n",
    "        alt.Chart(metrics.to_pandas())\n",
    "        .mark_bar(width=10)\n",
    "        .encode(\n",
    "            x=\"reward\",\n",
    "            y=metric_name,\n",
    "            color=\"model\",\n",
    "            tooltip=[metric_name, \"model\", \"reward\"],\n",
    "        )\n",
    "        .facet(column=\"model\", row=\"task\")\n",
    "        .properties(title=\"F1 score\")\n",
    "    )\n",
    "\n",
    "\n",
    "plot_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make the plot above but compute the metrics separately based on different metadata values, e.g. `is_photorealistic`. For brevity, we only plot the best-performing model+reward combination per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-e1c5e3d0fd4840f7804166ad6bc499d5.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-e1c5e3d0fd4840f7804166ad6bc499d5.vega-embed details,\n",
       "  #altair-viz-e1c5e3d0fd4840f7804166ad6bc499d5.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-e1c5e3d0fd4840f7804166ad6bc499d5\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-e1c5e3d0fd4840f7804166ad6bc499d5\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-e1c5e3d0fd4840f7804166ad6bc499d5\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-2b6ba9cd7de6ad4a73ba4f910fed0ec7\"}, \"facet\": {\"column\": {\"field\": \"is_photorealistic\", \"type\": \"nominal\"}, \"row\": {\"field\": \"task\", \"type\": \"nominal\"}}, \"spec\": {\"mark\": {\"type\": \"bar\", \"width\": 10}, \"encoding\": {\"color\": {\"field\": \"model\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"f1\", \"type\": \"quantitative\"}, {\"field\": \"model\", \"type\": \"nominal\"}, {\"field\": \"reward\", \"type\": \"nominal\"}], \"x\": {\"field\": \"f1\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"evaluator\", \"type\": \"nominal\"}}, \"title\": \"F1 score\"}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-2b6ba9cd7de6ad4a73ba4f910fed0ec7\": [{\"task\": \"room_detection\", \"model\": \"gpt4\", \"reward\": \"default\", \"is_photorealistic\": true, \"f1\": 1.0, \"evaluator\": \"gpt4 + default\"}, {\"task\": \"room_detection\", \"model\": \"gpt4\", \"reward\": \"default\", \"is_photorealistic\": false, \"f1\": 1.0, \"evaluator\": \"gpt4 + default\"}, {\"task\": \"clip_through_detection\", \"model\": \"gpt4\", \"reward\": \"default\", \"is_photorealistic\": true, \"f1\": 0.4788732394366197, \"evaluator\": \"gpt4 + default\"}, {\"task\": \"clip_through_detection\", \"model\": \"gpt4\", \"reward\": \"default\", \"is_photorealistic\": false, \"f1\": 0.6510178645616951, \"evaluator\": \"gpt4 + default\"}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"logit\", \"is_photorealistic\": true, \"f1\": 0.7604275410727023, \"evaluator\": \"clip + logit\"}, {\"task\": \"room_detection\", \"model\": \"clip\", \"reward\": \"logit\", \"is_photorealistic\": false, \"f1\": 0.5319974143503555, \"evaluator\": \"clip + logit\"}, {\"task\": \"clip_through_detection\", \"model\": \"clip\", \"reward\": \"projection_0.50\", \"is_photorealistic\": true, \"f1\": 1.0, \"evaluator\": \"clip + projection_0.50\"}, {\"task\": \"clip_through_detection\", \"model\": \"clip\", \"reward\": \"projection_0.50\", \"is_photorealistic\": false, \"f1\": 0.453125, \"evaluator\": \"clip + projection_0.50\"}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"logit\", \"is_photorealistic\": true, \"f1\": 0.7427866541353382, \"evaluator\": \"viclip + logit\"}, {\"task\": \"room_detection\", \"model\": \"viclip\", \"reward\": \"logit\", \"is_photorealistic\": false, \"f1\": 0.7114052614052615, \"evaluator\": \"viclip + logit\"}, {\"task\": \"clip_through_detection\", \"model\": \"viclip\", \"reward\": \"logit\", \"is_photorealistic\": true, \"f1\": 0.4393939393939394, \"evaluator\": \"viclip + logit\"}, {\"task\": \"clip_through_detection\", \"model\": \"viclip\", \"reward\": \"logit\", \"is_photorealistic\": false, \"f1\": 0.42193548387096774, \"evaluator\": \"viclip + logit\"}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"logit\", \"is_photorealistic\": true, \"f1\": 0.325, \"evaluator\": \"s3d + logit\"}, {\"task\": \"room_detection\", \"model\": \"s3d\", \"reward\": \"logit\", \"is_photorealistic\": false, \"f1\": 0.33881578947368424, \"evaluator\": \"s3d + logit\"}, {\"task\": \"clip_through_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.0\", \"is_photorealistic\": true, \"f1\": 1.0, \"evaluator\": \"s3d + projection_0.0\"}, {\"task\": \"clip_through_detection\", \"model\": \"s3d\", \"reward\": \"projection_0.0\", \"is_photorealistic\": false, \"f1\": 0.40425531914893614, \"evaluator\": \"s3d + projection_0.0\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_per_photorealistic = predictions.group_by(\n",
    "    \"task\", \"model\", \"reward\", \"is_photorealistic\"\n",
    ").agg(\n",
    "    f1=compute_metric(f1),\n",
    "    # ...add more here!\n",
    ")\n",
    "\n",
    "best_models = metrics.group_by(\"task\", \"model\").agg(\n",
    "    pl.col(\"reward\").sort_by(\"f1\", descending=True).first()\n",
    ")\n",
    "\n",
    "# The metrics_per_photorealistic table filtered to only contain the best models\n",
    "# i.e. exactly one model+reward per task+is_photorealistic\n",
    "metrics_per_photorealistic = metrics_per_photorealistic.join(\n",
    "    best_models, on=[\"task\", \"model\", \"reward\"], how=\"semi\"\n",
    ").with_columns(evaluator=pl.concat_str(\"model\", \"reward\", separator=\" + \"))\n",
    "\n",
    "(\n",
    "    alt.Chart(metrics_per_photorealistic.to_pandas())\n",
    "    .mark_bar(width=10)\n",
    "    .encode(\n",
    "        x=\"f1\",\n",
    "        y=\"evaluator\",\n",
    "        color=\"model\",\n",
    "        tooltip=[\"f1\", \"model\", \"reward\"],\n",
    "    )\n",
    "    .properties(title=\"F1 score\")\n",
    "    .facet(row=\"task\", column=\"is_photorealistic\")\n",
    "    .resolve_scale(y=\"independent\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the plotting needs will probably be taken care of by the above, or small variations of it. Below we have the interactive confusion matrix; the code there shouldn't be too important to fully understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart(task, model):\n",
    "    # A table of best model+reard combination for each model and task\n",
    "    best_models = metrics.group_by(\"task\", \"model\").agg(\n",
    "        pl.col(\"reward\").sort_by(\"f1\", descending=True).first()\n",
    "    )\n",
    "\n",
    "    # The predictions table filtered to only contain the best models\n",
    "    # i.e. exactly one model+reward per task\n",
    "    best_model_predicitons = predictions.filter(\n",
    "        pl.col(\"task\") == task, pl.col(\"model\") == model\n",
    "    ).join(best_models, on=[\"task\", \"model\", \"reward\"], how=\"semi\")\n",
    "\n",
    "    true_label_size = best_model_predicitons.group_by(\n",
    "        \"model\", \"reward\", \"true_label\"\n",
    "    ).agg(pl.len().alias(\"true_label_size\"))\n",
    "\n",
    "    # A normal confusion matrix\n",
    "    confusion_matrix = (\n",
    "        best_model_predicitons.join(\n",
    "            true_label_size, on=[\"model\", \"reward\", \"true_label\"]\n",
    "        )\n",
    "        .group_by(\"model\", \"reward\", \"true_label\", \"predicted_label\")\n",
    "        .agg(count=pl.len(), ratio=pl.len() / pl.col(\"true_label_size\").first())\n",
    "    )\n",
    "\n",
    "    # Needed to register the click events\n",
    "    selection = alt.selection_point(\n",
    "        fields=[\"true_label\", \"predicted_label\"], name=\"selection\"\n",
    "    )\n",
    "\n",
    "    # Base chart to which we'll add layers later\n",
    "    base = (\n",
    "        alt.Chart(confusion_matrix.to_pandas())\n",
    "        .encode(\n",
    "            x=\"predicted_label\",\n",
    "            y=\"true_label\",\n",
    "        )\n",
    "        .properties(title=f\"{model}, {task}\")\n",
    "    )\n",
    "\n",
    "    # Heatmap layer\n",
    "    heatmap = base.mark_rect().encode(\n",
    "        color=alt.Color(\"ratio\").scale(scheme=\"blues\"),\n",
    "        tooltip=[\"true_label\", \"predicted_label\", \"count\", \"ratio\"],\n",
    "    )\n",
    "\n",
    "    # Diagonal frames layer\n",
    "    labels = confusion_matrix[\"true_label\"].unique()\n",
    "    diag_df = pl.DataFrame({\"predicted_label\": labels, \"true_label\": labels})\n",
    "    diagonal = (\n",
    "        alt.Chart(pl.DataFrame(diag_df).to_pandas())\n",
    "        .mark_rect(stroke=\"black\", strokeWidth=1, fillOpacity=0)\n",
    "        .encode(x=\"predicted_label\", y=\"true_label\")\n",
    "    )\n",
    "\n",
    "    # Text labels in cells\n",
    "    text = base.mark_text(baseline=\"middle\").encode(\n",
    "        alt.Text(\"ratio\", format=\".1~f\"),\n",
    "        color=alt.condition(\n",
    "            alt.datum.ratio < 0.5, alt.value(\"black\"), alt.value(\"white\")\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add the layers together and also add the click-selector from eariler\n",
    "    # Returning this would give us a normal chart, like the one above\n",
    "    chart = (heatmap + diagonal + text).add_params(selection)\n",
    "\n",
    "    # Wrap the chart in a Jupyter widget\n",
    "    jchart = alt.JupyterChart(chart)\n",
    "\n",
    "    # This is the vertical box the videos will live in\n",
    "    videos_widget = VBox()\n",
    "\n",
    "    # Click callback\n",
    "    def on_select(change):\n",
    "        if change.new.value is None:\n",
    "            return\n",
    "\n",
    "        paths = []\n",
    "\n",
    "        for sel in change.new.value:\n",
    "            # Get a list of videos that correspond to the cell that was clicked on\n",
    "            paths.extend(\n",
    "                best_model_predicitons.filter(\n",
    "                    pl.col(\"model\") == model,\n",
    "                    pl.col(\"task\") == task,\n",
    "                    pl.col(\"true_label\") == sel[\"true_label\"],\n",
    "                    pl.col(\"predicted_label\") == sel[\"predicted_label\"],\n",
    "                )\n",
    "                .get_column(\"path\")\n",
    "                .to_list()\n",
    "            )\n",
    "        # Load the videos based on the paths above, and put them into a flexbox\n",
    "        videos = []\n",
    "        for path in paths:\n",
    "            video = Video.from_file(path)\n",
    "            video.autoplay = True\n",
    "            video.loop = True\n",
    "            videos.append(VBox([video, Label(\"👆 \" + path)]))\n",
    "\n",
    "        videos_widget.children = videos\n",
    "\n",
    "    # Whenever the selection in the chart changes, call the callback above\n",
    "    jchart.selections.observe(on_select, [\"selection\"])\n",
    "\n",
    "    return HBox([jchart, videos_widget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the cell below, you can pick the model and task combination and also click the cells in the matrix to see which videos ended up in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52b2a58751547278535f16dbbcfef38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Task:', options=('clip_through_detection', 'room_detection'), value='clip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82ea15ae3aa47a6b11ee199c745fca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a task selection dropdown\n",
    "tasks = predictions.get_column(\"task\").unique().sort()\n",
    "task_dropdown = Dropdown(\n",
    "    options=tasks,\n",
    "    value=tasks[0],\n",
    "    description=\"Task:\",\n",
    ")\n",
    "\n",
    "# Create a model selection dropdown\n",
    "models = predictions.get_column(\"model\").unique().sort()\n",
    "model_dropdown = Dropdown(\n",
    "    options=models,\n",
    "    value=models[0],\n",
    "    description=\"Model:\",\n",
    ")\n",
    "\n",
    "# Create an \"output\", a sort of a canvas that we can render things into\n",
    "# This is needed for the live updates whenever the dropdowns change\n",
    "output = Output()\n",
    "\n",
    "\n",
    "def on_change(_change):\n",
    "    with output:\n",
    "        # Clear the canvas and render the new plot\n",
    "        clear_output()\n",
    "        display(chart(task_dropdown.value, model_dropdown.value))\n",
    "\n",
    "\n",
    "model_dropdown.observe(on_change, names=[\"value\"])\n",
    "task_dropdown.observe(on_change, names=[\"value\"])\n",
    "\n",
    "# Render the dropdowns in Jupyter\n",
    "display(VBox([task_dropdown, model_dropdown]))\n",
    "\n",
    "with output:\n",
    "    # Redner the chart into the output\n",
    "    display(chart(task_dropdown.value, model_dropdown.value))\n",
    "\n",
    "# Render the output in Jupyter\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1552276928.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    EDIT:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "EDIT:\n",
    "/data/datasets/habitat_recordings/2024-03-06/dining_room/dining_room_2.mp4\n",
    "/data/datasets/habitat_recordings/2024-03-06/kitchen/kitchen_5.mp4\n",
    "/data/datasets/habitat_recordings/2024-03-06/hall/hall_5.mp4\n",
    "/data/datasets/habitat_recordings/2024-03-11/2024-03-11_8.mp4\n",
    "/data/datasets/habitat_recordings/2024-03-06/office/office_2.mp4\n",
    "/data/datasets/habitat_recordings/2024-03-06/stairs/stairs_7.mp4\n",
    "/data/datasets/habitat_recordings/2024-03-11/2024-03-11_40.mp4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
